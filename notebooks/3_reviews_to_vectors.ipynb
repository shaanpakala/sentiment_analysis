{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff5fb617",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51e9db0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run '0_tools.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27209785",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_updated_word_dict = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c1bd8a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (use_updated_word_dict): word_dict = list(np.load(f'{train_vect_path}updated_word_dict.npy'))\n",
    "else: word_dict = list(np.load(f'{train_vect_path}word_dict.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "74e38287",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30111"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee39c0d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76034\n"
     ]
    }
   ],
   "source": [
    "# text = getFileText()\n",
    "# print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0e52e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_old = False\n",
    "\n",
    "if (clear_old): \n",
    "    vect_key = ''\n",
    "    start_text_index = 0\n",
    "    text_num=0\n",
    "    \n",
    "else: \n",
    "    old_np = np.load(f'{train_vect_path}train_vect.npy')\n",
    "    vect_key='new_'\n",
    "    start_text_index = old_np.shape[0]\n",
    "    text_num = start_text_index\n",
    "\n",
    "finished = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6262de6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(76034, 30113) + (0,)"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input arrays must have same number of dimensions, but the array at index 0 has 2 dimension(s) and the array at index 1 has 1 dimension(s)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m new_np \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_vect_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mvect_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mtrain_vect.npy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mold_np\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m + \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_np\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, end \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m final_np \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate((old_np, new_np), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinal_np\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m<__array_function__ internals>:200\u001b[0m, in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: all the input arrays must have same number of dimensions, but the array at index 0 has 2 dimension(s) and the array at index 1 has 1 dimension(s)"
     ]
    }
   ],
   "source": [
    "if (not clear_old):\n",
    "    new_np = np.load(f'{train_vect_path}{vect_key}train_vect.npy')\n",
    "    print(f'{old_np.shape} + {new_np.shape}', end = '')\n",
    "    final_np = np.concatenate((old_np, new_np), axis=0)\n",
    "    print(f' = {final_np.shape}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88dc2566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved (76034, 30113) array.\n"
     ]
    }
   ],
   "source": [
    "if (not clear_old):\n",
    "    np.save(f'{train_vect_path}train_vect.npy', final_np)\n",
    "    print(f'Successfully saved {final_np.shape} array.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90e94b25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting at article 76034/76036.\n",
      "76034/76036 vectors saved.\n"
     ]
    }
   ],
   "source": [
    "text = getFileText()\n",
    "\n",
    "dict_indices = [i for i in range(len(word_dict))]\n",
    "\n",
    "list_1 = []\n",
    "\n",
    "total_texts = len(text)\n",
    "\n",
    "if (start_text_index == 0): print('Starting at beginning of text corpus.')\n",
    "else: print(f'Starting at article {start_text_index}/{total_texts}.')\n",
    "    \n",
    "start_lap = time.time()\n",
    "for review_rating in text[start_text_index:]:\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        rr = review_rating.split('\\n - \\n')\n",
    "        \n",
    "        rating = rr[0]\n",
    "        \n",
    "        review = rr[1]\n",
    "        \n",
    "        review_len = len(review)\n",
    "        \n",
    "        if (review_len < 7): continue\n",
    "\n",
    "        vector = vectorize(review, word_dict)\n",
    "        \n",
    "        if (int(rating)<=2): r = -1\n",
    "        elif (int(rating)==3): r = 0\n",
    "        elif (int(rating)>=4): r = 1\n",
    "            \n",
    "        list_1.append([int(r), review_len] + vector)\n",
    "\n",
    "        text_num+=1\n",
    "        if ((text_num-start_text_index)%1000==0):\n",
    "                        \n",
    "            end_lap = time.time()\n",
    "            np_list = np.array(list_1)\n",
    "            save_start = time.time()\n",
    "#             np.save(f'{train_vect_path}{vect_key}train_vect.npy', np_list)\n",
    "            save_end = time.time()\n",
    "            \n",
    "            elapsed_time = end_lap - start_lap\n",
    "            \n",
    "            print(f'{text_num-start_text_index}/{total_texts-start_text_index} vectors saved', end = ' ')\n",
    "            print(f'| {str(elapsed_time)[:5]} time units | Save Time {str(save_end-save_start)[:5]} time units.')\n",
    "            \n",
    "            start_lap = time.time()\n",
    "            \n",
    "        if(text_num == total_texts-1): \n",
    "            np_list = np.array(list_1)\n",
    "            \n",
    "            np.save(f'{train_vect_path}{vect_key}train_vect.npy', np_list)\n",
    "            print(f'{text_num-start_text_index}/{total_texts-start_text_index} vectors saved.')\n",
    "            finished = True\n",
    "        \n",
    "    except: continue\n",
    "\n",
    "if (not finished):\n",
    "    np_list = np.array(list_1)\n",
    "    np.save(f'{train_vect_path}{vect_key}train_vect.npy', np_list)\n",
    "    print(f'{text_num}/{total_texts} vectors saved.')\n",
    "    finished = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cf9ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (not finished):\n",
    "    np_list = np.array(list_1)\n",
    "    np.save(f'{train_vect_path}{vect_key}train_vect.npy', np_list)\n",
    "    print(f'{text_num+1}/{total_texts} vectors saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a37f477",
   "metadata": {},
   "source": [
    "### remove words from the dictionary that do not appear much throughout the corpus\n",
    "ideally misspelled words, words in other languages, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a388d787",
   "metadata": {},
   "outputs": [],
   "source": [
    "prune=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62d8409e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold = 0.0005: 37270 values.\n",
      "Removed words, 30111/67379 remain.\n"
     ]
    }
   ],
   "source": [
    "if(prune):\n",
    "    \n",
    "    more_low_sums = False\n",
    "    prune_df = pd.DataFrame(np.load(f'{train_vect_path}train_vect.npy')[:40000])\n",
    "\n",
    "    sums=[1, 1]\n",
    "\n",
    "    for i in range(2, prune_df.shape[1]):\n",
    "        sums.append(prune_df[i].sum())\n",
    "    \n",
    "    threshold = 0.0005\n",
    "    low_sums = []\n",
    "    \n",
    "    for s in sums: \n",
    "        if (s<threshold): low_sums.append(s)\n",
    "        \n",
    "    if (more_low_sums):\n",
    "        \n",
    "        threshold1 = threshold*2\n",
    "        threshold2 = threshold/2\n",
    "        \n",
    "        low_sums_1 = []\n",
    "        low_sums_2 = []\n",
    "        \n",
    "        for s in sums:\n",
    "            if (s<threshold1): low_sums_1.append(s)\n",
    "            if (s<threshold2): low_sums_2.append(s)\n",
    "            \n",
    "        print(f'Threshold = {threshold1}: {len(low_sums_1)} values.')\n",
    "        print(f'Threshold = {threshold}: {len(low_sums)} values.')\n",
    "        print(f'Threshold = {threshold2}: {len(low_sums_2)} values.')\n",
    "        \n",
    "    else: print(f'Threshold = {threshold}: {len(low_sums)} values.')\n",
    "    \n",
    "    prune_df_sz = prune_df.shape[0]\n",
    "    prune_df.loc[prune_df_sz] = sums\n",
    "    \n",
    "    columns_remove = (prune_df < threshold).loc[prune_df_sz, 2:]\n",
    "    columns_remove = pd.concat([pd.Series([False, False]), columns_remove])\n",
    "    prune_df = prune_df.drop(columns = prune_df.columns[columns_remove])\n",
    "\n",
    "    \n",
    "    update_word_dict = set()\n",
    "\n",
    "    for word_index in (prune_df.columns.tolist()):\n",
    "        word = word_dict[word_index-2]\n",
    "        update_word_dict.add(word)\n",
    "\n",
    "    update_word_dict_np = np.array(list(update_word_dict))\n",
    "    np.save(f'{train_vect_path}updated_word_dict.npy', update_word_dict_np)\n",
    "    print(f'Removed words, {update_word_dict_np.shape[0]}/{len(word_dict)} remain.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4843ef48",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
