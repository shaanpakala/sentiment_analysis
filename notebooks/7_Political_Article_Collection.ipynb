{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd8db10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plot\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce084eef",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run '0_tools.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd838fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/70.0.3538.77 Safari/537.36'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24fa3017",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords = ['and', 'at', 'the', 'yet', 'so', 'because', 'on', 'of', 'to', 'as',' in', 'his', 'her', 'she', 'him',\n",
    "            'them', 'they', 'it', 'hers', 'their', 'its', 'theirs', 'with', 'said', 'for', 'after', 'will', 'that',\n",
    "            'about', 'who', 'by', 'all', 'where', 'over', 'year', 'years', 'continue', 'two', 'three', 'four', 'five',\n",
    "            'six', 'seven', 'eight', 'nine', 'ten', 'other', 'into']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "12bf2551",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trimWord(word):\n",
    "    w = word\n",
    "    \n",
    "    if ((w[len(w)-1] == \",\")or(w[len(w)-1] == \".\")or(w[len(w)-1] == \"?\")\n",
    "        or(w[len(w)-1] == \"!\")or(w[len(w)-1] == \"’\")or(w[len(w)-1] == \":\")\n",
    "        or(w[len(w)-1]==\"\\\"\")or(w[len(w)-1]==\"-\")or(w[len(w)-1]==\"'\")):\n",
    "        w = w[:len(w)-1]\n",
    "                        \n",
    "    if((w[0]==\"'\")or(w[0]==\"’\")or(w[0]==\"\\\"\")or(w[0]==\"-\")):\n",
    "        w = w[1:]\n",
    "                        \n",
    "    if ((w[len(w)-2:len(w)] == \"’s\")or(w[len(w)-2:len(w)] == \"\\'s\")):\n",
    "        w = w[:len(w)-2]\n",
    "    \n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7906bbe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapeArticlesCNN(w, rBank, dBank):\n",
    "    dataFile = open(f\"{data_path}political_articles/cnn.txt\", \"a\")\n",
    "    \n",
    "    www = w\n",
    "    www = requests.get(www,headers = headers)\n",
    "\n",
    "    if (www.status_code != 200):\n",
    "        return \"Error\"\n",
    "\n",
    "    soup = BeautifulSoup(www.text, 'html.parser')    \n",
    "    \n",
    "    headlineBlocks = soup.find_all('span', class_ = 'sitemap-link')\n",
    "    headlineBlocks = headlineBlocks[1:]\n",
    "    #for i in headlineBlocks:\n",
    "    #    print(i)\n",
    "    x=0\n",
    "    for currentHeadline in headlineBlocks:\n",
    "        x+=1\n",
    "        articleWWW = str(currentHeadline.find('a')).split(\"\\\"\")[1]\n",
    "        #print(articleWWW)\n",
    "        articleWWW = requests.get(articleWWW,headers = headers)\n",
    "        \n",
    "        if (www.status_code != 200):\n",
    "            return \"Error\"\n",
    "        \n",
    "        soup = BeautifulSoup(articleWWW.text, 'html.parser')\n",
    "        \n",
    "        \n",
    "        #-------------------------------------------------------------------------------------------------------------\n",
    "        headline = soup.find('h1', class_=\"headline__text inline-placeholder\")\n",
    "        #print(headline.text.split())\n",
    "        \n",
    "        r = False\n",
    "        d = False\n",
    "        try:\n",
    "            headline_words = headline.text.split()\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "            \n",
    "        for word in headline_words:\n",
    "            try:\n",
    "                word = trimWord(word)\n",
    "                word = word.lower()\n",
    "            except:\n",
    "                continue\n",
    "                \n",
    "            if(word in stopWords):\n",
    "                continue\n",
    "            \n",
    "            if (not r):\n",
    "                r = (word in rBank)\n",
    "            if (not d):\n",
    "                d = (word in dBank)\n",
    "            \n",
    "            \n",
    "        if (not ((r or d) & (not (r&d)))):\n",
    "            continue\n",
    "        #------------------------------------------------------------------------------------------------------------- \n",
    "        paragraph = soup.find_all('p', class_ = 'paragraph inline-placeholder')\n",
    "        \n",
    "        try:\n",
    "            if (r):\n",
    "                dataFile.write(paragraph[0].text + paragraph[1].text + paragraph[2].text + ' r     \\n')\n",
    "            if (d):\n",
    "                dataFile.write(paragraph[0].text + paragraph[1].text + paragraph[2].text + ' d     \\n')\n",
    "        except:\n",
    "            continue\n",
    "        \n",
    "\n",
    "    dataFile.close()\n",
    "    \n",
    "    return \"done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f2588191",
   "metadata": {},
   "outputs": [],
   "source": [
    "www = \"https://www.multistate.us/resources/2023-governors-and-legislatures\"\n",
    "\n",
    "www = requests.get(www,headers = headers)\n",
    "if (www.status_code != 200):\n",
    "    print(\"Error\")\n",
    "\n",
    "soup = BeautifulSoup(www.text, 'html.parser')\n",
    "\n",
    "politicians = []\n",
    "parties = []\n",
    "\n",
    "govBlocks = soup.find_all(\"td\", class_ = \"gov left\")\n",
    "\n",
    "for govBlock in govBlocks:\n",
    "    \n",
    "    vect = govBlock.text.split()\n",
    "    governor = vect[1]\n",
    "    party = vect[len(vect)-1][1]\n",
    "    \n",
    "    if(party==\"D\"):\n",
    "        party = \"Democratic\"\n",
    "    if(party==\"R\"):\n",
    "        party = \"Republican\"\n",
    "            \n",
    "    politicians.append(governor.lower())\n",
    "    parties.append(party)\n",
    "\n",
    "www = \"https://ballotpedia.org/List_of_current_members_of_the_U.S._Congress\"\n",
    "\n",
    "www = requests.get(www,headers = headers)\n",
    "if (www.status_code != 200):\n",
    "    print(\"Error\")\n",
    "\n",
    "soup = BeautifulSoup(www.text, 'html.parser')\n",
    "        \n",
    "people = soup.find_all('tr')\n",
    "\n",
    "for i in people:\n",
    "    try:\n",
    "        name = i.find('td', style=\"padding-left:10px;text-align:center;\").text\n",
    "        party = \"\"\n",
    "        try:\n",
    "            party = i.find(\"td\", class_ = \"partytd Democratic\").text\n",
    "            party = \"Democratic\"\n",
    "        except:\n",
    "            party = i.find(\"td\", class_ = \"partytd Republican\").text\n",
    "            party = \"Republican\"\n",
    "        \n",
    "        names = name.split()\n",
    "        last_name = names[len(names)-1]\n",
    "        politicians.append(last_name.lower())\n",
    "        parties.append(party)\n",
    "        \n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d92f04e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "additionalRepublicans = ['trump', 'g.o.p.', 'gop', 'republicans', 'republican', 'conservatives', 'pence', 'herschel']\n",
    "additionalDemocrats = ['pelosi', 'biden', 'democrat', 'democrats', 'liberals', 'kamala', 'obama', 'bernie', 'clinton']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "9ecf9b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "people = \"\"\n",
    "pdf = pd.DataFrame()\n",
    "pdf['Politician'] = politicians\n",
    "pdf['Party'] = parties\n",
    "\n",
    "for a in additionalRepublicans:\n",
    "    pdf.loc[len(pdf['Party'].values)] = a.lower()\n",
    "    pdf['Party'].loc[len(pdf['Party'].values)-1] = 'Republican'\n",
    "\n",
    "for b in additionalDemocrats:\n",
    "    pdf.loc[len(pdf['Party'].values)] = b.lower()\n",
    "    pdf['Party'].loc[len(pdf['Party'].values)-1] = 'Democratic'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "82f53857",
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove some politicians with generic names\n",
    "removePoliticians = ['justice', 'cooper', 'cox', 'murphy', 'green', 'greene', 'little', 'moore', 'reeves', 'shapiro', \n",
    "                     'lee', 'scott', 'jr.', 'miller', 'rodgers', 'good', 'fitzgerald', 'smith', 'tiffany', 'carson', \n",
    "                    'stevens', 'james', 'harris', 'davis', 'carter', 'clark', 'jackson', 'collins', 'bush', 'thompson'\n",
    "                    'gordon', 'young', 'paul', 'peters', 'brown', 'murray', 'iii', 'johnson' 'carl' 'rogers', 'strong',\n",
    "                    'crane', 'hill', 'kim', 'steel', 'crow', 'courtney', 'bean', 'frost', 'posey', 'wilson', 'williams',\n",
    "                    'allen', 'simpson', 'flood', 'ryan', 'ross', 'adams', 'edwards', 'jordan', 'lucas', 'cole', 'rose',\n",
    "                    'fallon', 'roy', 'drew', 'clarke', 'wild', 'gordon', 'kennedy', 'schmitt', 'booker', 'vance', \n",
    "                     'mullin', 'reed', 'graham', 'evans', 'dean', 'duncan', 'case', 'hunt', 'golden', 'carey', 'joyce',\n",
    "                    'johnson','graves', 'davidson', 'franklin', 'cassidy', 'garcia', 'mills', 'thompson', 'curtis',\n",
    "                    'stewart', 'guest', 'cloud', 'banks', 'self', 'waters', 'buck', 'game', 'foster', 'rounds', 'carl',\n",
    "                    'kiley', 'mast', 'rogers', 'owens', 'fry', 'norman', 'crenshaw']\n",
    "\n",
    "for r in removePoliticians:\n",
    "    try:\n",
    "        pdf = pdf.drop(pdf.index[pdf['Politician'] == r.lower()].tolist())\n",
    "        \n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "748699e1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "republicans = list(pdf[pdf['Party'] == 'Republican']['Politician'])\n",
    "democrats = list(pdf[pdf['Party'] == 'Democratic']['Politician'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "9689fe30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(f'{data_path}political_articles/republicans.npy', np.array(republicans))\n",
    "# np.save(f'{data_path}political_articles/democrats.npy', np.array(democrats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "95a28efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "republicans = list(np.load(f'{data_path}political_articles/republicans.npy'))\n",
    "democrats = list(np.load(f'{data_path}political_articles/democrats.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c9cb00b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "startDate = '10/2022'\n",
    "endDate = '04/2023'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12fbc54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "startDate = startDate.split('/')\n",
    "endDate = endDate.split('/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd49e49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "startMonth = int(startDate[0])\n",
    "startYear = int(startDate[1])\n",
    "\n",
    "endMonth = int(endDate[0])\n",
    "endYear = int(endDate[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0c957519",
   "metadata": {},
   "outputs": [],
   "source": [
    "months = ['no', 'January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', \n",
    "          'October', 'November', 'December']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f1b7a9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def daysInMonth(m, y):\n",
    "    if(m==2): \n",
    "        if (y%100==0):\n",
    "            if(y%400==0): return 29\n",
    "            else: return 28\n",
    "        elif(y%4==0): return 29\n",
    "        else: return 28\n",
    "    elif ((m==1)or(m==3)or(m==5)or(m==7)or(m==8)or(m==10)or(m==12)): return 31\n",
    "    else: return 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2eb51969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# open(f'{data_path}political_articles/cnn.txt', 'w').close()\n",
    "\n",
    "# for year in range(startYear, endYear+1):\n",
    "    \n",
    "#     if (year == startYear):\n",
    "#         beginM = startMonth\n",
    "#     else:\n",
    "#         beginM = 1\n",
    "        \n",
    "#     if (year == endYear):\n",
    "#         endM = endMonth\n",
    "#     else:\n",
    "#         endM = 12\n",
    "    \n",
    "#     for month in range(beginM,endM+1):\n",
    "#         print(f\"{months[month]} {year}\")\n",
    "#         print(scrapeArticlesCNN(f\"https://www.cnn.com/politics/article/sitemap-{str(year)}-{str(month)}.html\", republicans, democrats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a14da662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def scrapeArticlesWSJ(w, rBank, dBank):\n",
    "#     dataFile = open(f\"{data_path}political_articles/WSJ.txt\", \"a\")\n",
    "    \n",
    "#     print(w, '\\n')\n",
    "    \n",
    "#     www = w\n",
    "#     www = requests.get(www,headers = headers)\n",
    "\n",
    "#     if (www.status_code != 200):\n",
    "#         return \"Error\"\n",
    "\n",
    "#     soup = BeautifulSoup(www.text, 'html.parser')    \n",
    "    \n",
    "#     articleBlocks = soup.find_all('article')\n",
    "    \n",
    "#     for articleBlock in articleBlocks:\n",
    "#         r=False\n",
    "#         d=False\n",
    "#         bloc = articleBlock.find('a')\n",
    "# #         print(bloc.find('span').text)\n",
    "#         bloc = (str(bloc).split(\"\\\"\"))\n",
    "    \n",
    "#         articleWWW = 'Error'\n",
    "        \n",
    "#         for b in bloc:\n",
    "#             if (b[:8]==\"https://\"): \n",
    "#                 articleWWW = b\n",
    "#                 break\n",
    "                \n",
    "#         if (articleWWW == 'Error'): continue\n",
    "                \n",
    "#         headlineWords = (articleWWW[29:]).split('-')\n",
    "#         for word in headlineWords:\n",
    "#             if (word in rBank): r=True\n",
    "#             if (word in dBank): d=True\n",
    "#             if (r&d): break\n",
    "                \n",
    "#         if (not ((r or d) & (not (r&d)))): continue\n",
    "         \n",
    "#  #     ____________________________________________________________________________________________________\n",
    "         \n",
    "        \n",
    "#         print(articleWWW)\n",
    "        \n",
    "#         articleWWW = requests.get(articleWWW,headers = headers)\n",
    "\n",
    "#         if (articleWWW.status_code != 200):\n",
    "#             return \"Error\"\n",
    "\n",
    "#         articleSoup = BeautifulSoup(articleWWW.text, 'html.parser')    \n",
    "#         print(articleSoup.text)\n",
    "        \n",
    "#         break\n",
    "#  #     ____________________________________________________________________________________________________\n",
    "                \n",
    "#     dataFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "78cc11c3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# startDate = '10/2022'\n",
    "# endDate = '11/2022'\n",
    "\n",
    "# startDate = startDate.split('/')\n",
    "# endDate = endDate.split('/')\n",
    "\n",
    "# startMonth = int(startDate[0])\n",
    "# startYear = int(startDate[1])\n",
    "\n",
    "# endMonth = int(endDate[0])\n",
    "# endYear = int(endDate[1])\n",
    "\n",
    "# open(f'{data_path}political_articles/WSJ.txt', 'w').close()\n",
    "\n",
    "# for year in range(startYear, endYear+1):\n",
    "    \n",
    "#     if (year == startYear):\n",
    "#         beginM = startMonth\n",
    "#     else: beginM = 1\n",
    "        \n",
    "#     if (year == endYear):\n",
    "#         endM = endMonth\n",
    "#     else: endM = 12\n",
    "\n",
    "#     for month in range(beginM, endM+1):\n",
    "#         endDay = daysInMonth(month, year)\n",
    "#         for day in range(1, endDay+1):\n",
    "#             if (day < 10): d = '0' + str(day)\n",
    "#             else: d = str(day)\n",
    "#             scrapeArticlesWSJ(f'https://www.wsj.com/news/archive/{str(year)}/{str(month)}/{d}', republicans, democrats)\n",
    "            \n",
    "#             break\n",
    "#         break\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "42444d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrapeWashingtonExaminer(w, rBank, dBank):\n",
    "    dataFile = open(f\"{data_path}political_articles/WashExam.txt\", \"a\")\n",
    "    \n",
    "#     print(w, '\\n')\n",
    "    \n",
    "    www = w\n",
    "    www = requests.get(www,headers = headers)\n",
    "\n",
    "    if (www.status_code != 200):\n",
    "        return \"Error\"\n",
    "\n",
    "    soup = BeautifulSoup(www.text, 'html.parser')    \n",
    "    \n",
    "    articleBlocks = soup.find_all('li', class_='ArchivePage-items-item link-item')\n",
    "    \n",
    "    links = []\n",
    "    \n",
    "    for articleBlock in articleBlocks:\n",
    "        headline = (str(articleBlock.text))\n",
    "        r=False\n",
    "        d=False\n",
    "        for i in headline.split():\n",
    "            word = i.lower()\n",
    "            if (word in rBank): r=True\n",
    "            if (word in dBank): d=True\n",
    "            if (r&d): break\n",
    "                \n",
    "#         if(r&d): print(headline)\n",
    "        if (not ((r or d) & (not (r&d)))): continue\n",
    "            \n",
    "        articleWWW = articleBlock.find('a')['href']\n",
    "        \n",
    "        if(articleWWW.find(' ')!=-1): continue\n",
    "                    \n",
    "                \n",
    "        articleWWW = requests.get(articleWWW,headers = headers)\n",
    "\n",
    "        if (articleWWW.status_code != 200):\n",
    "            return \"Error\"\n",
    "\n",
    "        articleSoup = BeautifulSoup(articleWWW.text, 'html.parser')    \n",
    "        try:\n",
    "            text_blocks = (articleSoup.find('div', class_='RichTextArticleBody-body')).find_all('p')\n",
    "        except: continue\n",
    "            \n",
    "        num_t = 0\n",
    "        finalText = ''\n",
    "        for i in text_blocks: \n",
    "            if(num_t >= 3): break\n",
    "            text = i.text\n",
    "            if (text.isupper()): continue\n",
    "        \n",
    "            finalText += text\n",
    "            num_t+=1\n",
    "            \n",
    "        if (r): dataFile.write(finalText + ' r     \\n')\n",
    "        elif(d): dataFile.write(finalText + ' d     \\n')        \n",
    "    \n",
    "    \n",
    "    dataFile.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "9f9d6b71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "October 2022\n",
      "November 2022\n",
      "December 2022\n",
      "January 2023\n",
      "February 2023\n",
      "March 2023\n",
      "April 2023\n"
     ]
    }
   ],
   "source": [
    "startDate = '10/2022'\n",
    "endDate = '04/2023'\n",
    "\n",
    "startDate = startDate.split('/')\n",
    "endDate = endDate.split('/')\n",
    "\n",
    "startMonth = int(startDate[0])\n",
    "startYear = int(startDate[1])\n",
    "\n",
    "endMonth = int(endDate[0])\n",
    "endYear = int(endDate[1])\n",
    "\n",
    "open(f'{data_path}political_articles/WashExam.txt', 'w').close()\n",
    "\n",
    "for year in range(startYear, endYear+1):\n",
    "    \n",
    "    if (year == startYear):\n",
    "        beginM = startMonth\n",
    "    else: beginM = 1\n",
    "        \n",
    "    if (year == endYear):\n",
    "        endM = endMonth\n",
    "    else: endM = 12\n",
    "\n",
    "    for month in range(beginM, endM+1):\n",
    "        print(f\"{months[month]} {year}\", end = '')\n",
    "        endDay = daysInMonth(month, year)\n",
    "        \n",
    "        for day in range(1, endDay+1):\n",
    "            scrapeWashingtonExaminer(f'https://www.washingtonexaminer.com/sitemap/{str(year)}/{str(month)}/{str(day)}', republicans, democrats)         \n",
    "            \n",
    "        print('done.')\n",
    "\n",
    "print('All done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59edd23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
