{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff5fb617",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51e9db0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run '0_tools.ipynb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27209785",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_updated_word_dict = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1bd8a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (use_updated_word_dict):\n",
    "    word_dict = list(np.load(f'{train_vect_path}updated_word_dict.npy'))\n",
    "else:\n",
    "    word_dict = list(np.load(f'{train_vect_path}word_dict.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74e38287",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35797"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95907bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = getFileText()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0e4010b9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d0e52e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_old = False\n",
    "\n",
    "if (clear_old): \n",
    "    vect_key = ''\n",
    "    start_text_index = 0\n",
    "    text_num=0\n",
    "else: \n",
    "    old_np = np.load(f'{train_vect_path}train_vect.npy')\n",
    "    vect_key='new_'\n",
    "    start_text_index = old_np.shape[0]\n",
    "    text_num = start_text_index\n",
    "\n",
    "finished = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b6262de6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(66974, 35799) + (9051, 35799) = (76025, 35799)\n"
     ]
    }
   ],
   "source": [
    "if (not clear_old):\n",
    "    new_np = np.load(f'{train_vect_path}{vect_key}train_vect.npy')\n",
    "#     final_np = np.concatenate((old_np, new_np), axis=0)\n",
    "    final_np = np.array(list(old_np) + list(new_np))\n",
    "    np.save(f'{train_vect_path}train_vect.npy', final_np)\n",
    "    print(f'{old_np.shape} + {new_np.shape} = {final_np.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90e94b25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting at article 66974/76036.\n",
      "1000/9062 vectors saved | 4.9340 time units.\n",
      "2000/9062 vectors saved | 6.0103 time units.\n",
      "3000/9062 vectors saved | 5.6515 time units.\n",
      "4000/9062 vectors saved | 4.4693 time units.\n",
      "5000/9062 vectors saved | 5.1663 time units.\n",
      "6000/9062 vectors saved | 7.5706 time units.\n",
      "7000/9062 vectors saved | 11.240 time units.\n",
      "8000/9062 vectors saved | 7.0490 time units.\n",
      "9000/9062 vectors saved | 5.9354 time units.\n",
      "76025/76036 vectors saved.\n"
     ]
    }
   ],
   "source": [
    "dict_indices = [i for i in range(len(word_dict))]\n",
    "\n",
    "list_1 = []\n",
    "\n",
    "total_texts = len(text)\n",
    "\n",
    "if (start_text_index == 0): print('Starting at beginning of text corpus.')\n",
    "else: print(f'Starting at article {start_text_index}/{total_texts}.')\n",
    "    \n",
    "start_lap = time.time()\n",
    "for review_rating in text[start_text_index:]:\n",
    "    \n",
    "    if (len(review_rating) < 7): continue\n",
    "    try:\n",
    "        \n",
    "        rr = review_rating.split('\\n - \\n')\n",
    "        \n",
    "        rating = rr[0]\n",
    "        \n",
    "        review = rr[1]\n",
    "\n",
    "        vector = [0 for v in range(len(word_dict))]\n",
    "                \n",
    "        for i in review.split():\n",
    "            try:\n",
    "                word = trimWord(i).lower()\n",
    "                \n",
    "                if (word in stopWords): continue\n",
    "\n",
    "                randomchar = False\n",
    "\n",
    "                for c in randomchars: \n",
    "                    if (word.find(c) != -1): randomchar = True\n",
    "\n",
    "                if (randomchar): continue\n",
    "                    \n",
    "                if (not hasVowel(word)): continue\n",
    "\n",
    "                vector[word_dict.index(word)]+=1\n",
    "                \n",
    "            except:\n",
    "                continue\n",
    "            \n",
    "            \n",
    "        length = sum(vector)\n",
    "        new_vector = [x / length for x in vector]\n",
    "        \n",
    "        \n",
    "        if (int(rating)<=2): r = -1\n",
    "        elif (int(rating)==3): r = 0\n",
    "        elif (int(rating)>=4): r = 1\n",
    "            \n",
    "        list_1.append([int(r), length] + new_vector)\n",
    "\n",
    "        text_num+=1\n",
    "        if ((text_num-start_text_index)%1000==0):\n",
    "                        \n",
    "            end_lap = time.time()\n",
    "            np_list = np.array(list_1)\n",
    "            save_start = time.time()\n",
    "            np.save(f'{train_vect_path}{vect_key}train_vect.npy', np_list)\n",
    "            save_end = time.time()\n",
    "            \n",
    "            elapsed_time = end_lap - start_lap\n",
    "            \n",
    "            \n",
    "            print(f'{text_num-start_text_index}/{total_texts-start_text_index} vectors saved', end = ' ')\n",
    "            print(f'| {str(elapsed_time)[:6]} time units | Save Time {save_end-save_start} time units.')\n",
    "            \n",
    "            start_lap = time.time()\n",
    "            \n",
    "        if(text_num == total_texts-1): \n",
    "            np_list = np.array(list_1)\n",
    "            \n",
    "            np.save(f'{train_vect_path}{vect_key}train_vect.npy', np_list)\n",
    "            print(f'{text_num+1-start_text_index}/{total_texts-start_text_index} vectors saved.')\n",
    "            finished = True\n",
    "        \n",
    "    except: continue\n",
    "\n",
    "if (not finished):\n",
    "    np_list = np.array(list_1)\n",
    "    np.save(f'{train_vect_path}{vect_key}train_vect.npy', np_list)\n",
    "    print(f'{text_num}/{total_texts} vectors saved.')\n",
    "    finished = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cf9ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if (not finished):\n",
    "    np_list = np.array(list_1)\n",
    "    np.save(f'{train_vect_path}{vect_key}train_vect.npy', np_list)\n",
    "    print(f'{text_num}/{total_texts} vectors saved.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a37f477",
   "metadata": {},
   "source": [
    "###### remove words from the dictionary that do not appear much throughout the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a388d787",
   "metadata": {},
   "outputs": [],
   "source": [
    "prune=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "62d8409e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold = 0.01: 27011 values.\n",
      "Removed words, 35797/62808 remain.\n"
     ]
    }
   ],
   "source": [
    "if(prune):\n",
    "    \n",
    "    more_low_sums = False\n",
    "    prune_df = pd.DataFrame(np.load(f'{train_vect_path}train_vect.npy'))\n",
    "\n",
    "    sums=[1, 1]\n",
    "\n",
    "    for i in range(2, prune_df.shape[1]):\n",
    "        sums.append(prune_df[i].sum())\n",
    "    \n",
    "    threshold = 0.01\n",
    "    low_sums = []\n",
    "    \n",
    "    for s in sums: \n",
    "        if (s<threshold): low_sums.append(s)\n",
    "        \n",
    "    if (more_low_sums):\n",
    "        \n",
    "        threshold1 = threshold*2\n",
    "        threshold2 = threshold/2\n",
    "        \n",
    "        low_sums_1 = []\n",
    "        low_sums_2 = []\n",
    "        \n",
    "        for s in sums:\n",
    "            if (s<threshold1): low_sums_1.append(s)\n",
    "            if (s<threshold2): low_sums_2.append(s)\n",
    "            \n",
    "        print(f'Threshold = {threshold1}: {len(low_sums_1)} values.')\n",
    "        print(f'Threshold = {threshold}: {len(low_sums)} values.')\n",
    "        print(f'Threshold2 = {threshold2}: {len(low_sums_2)} values.')\n",
    "        \n",
    "    else: print(f'Threshold = {threshold}: {len(low_sums)} values.')\n",
    "    \n",
    "    prune_df_sz = prune_df.shape[0]\n",
    "    prune_df.loc[prune_df_sz] = sums\n",
    "    \n",
    "    columns_remove = (prune_df < threshold).loc[prune_df_sz, 2:]\n",
    "    columns_remove = pd.concat([pd.Series([False, False]), columns_remove])\n",
    "    prune_df = prune_df.drop(columns = prune_df.columns[columns_remove])\n",
    "    \n",
    "    \n",
    "    update_word_dict = set()\n",
    "\n",
    "    for word_index in (prune_df.columns.tolist()):\n",
    "        word = word_dict[word_index-2]\n",
    "        update_word_dict.add(word)\n",
    "\n",
    "    update_word_dict_np = np.array(list(update_word_dict))\n",
    "    np.save(f'{train_vect_path}updated_word_dict.npy', update_word_dict_np)\n",
    "    print(f'Removed words, {update_word_dict_np.shape[0]}/{len(word_dict)} remain.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
